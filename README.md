# Encryption-friendly LLM Architecture

This project implements Fully Homomorphic Encryption (FHE) compatible versions of language models, allowing secure inference on encrypted data. We've successfully created transformer architectures that can run under FHE while maintaining model quality.

## Working FHE Models

### 1. FHE-Reduced Bit-width GPT (`fhe_nanogpt_reduced_bitwidth.py`)
A transformer-based language model that successfully runs under FHE with the following capabilities:
- 3 transformer layers
- 64-dimensional embeddings
- 32 token context window
- 256 vocabulary size
- 0.13M parameters
- Perfect accuracy under FHE (MAE: ~1.77e-17)

Key innovations:
- Explicit bit-width reduction between layers
- Modified residual connections using averaging
- Aggressive quantization (3-bit)
- FHE-friendly activation functions

### 2. Minimal FHE Models (`fhe_nanogpt_concrete_v3.py`)
Reference implementations showing FHE compatibility:
- MinimalFHEModel: Basic FHE-compatible neural network
- SimplifiedTransformerBlock: Single transformer block with FHE optimizations

## Running the Models

### Prerequisites
1. Python 3.10
2. PyTorch
3. Concrete-ML (will be installed automatically)

### Setup
```bash
# Create and activate a Python 3.10 virtual environment
python3.10 -m venv fhe_llm_env_py310
source fhe_llm_env_py310/bin/activate

# Install dependencies
pip install torch numpy
```

### Running FHE-Reduced Bit-width GPT
```bash
python fhe_nanogpt_reduced_bitwidth.py
```
This will:
1. Create a FHE-compatible GPT model
2. Test regular inference
3. Compile for FHE
4. Run in FHE simulation mode
5. Run with actual encryption

### Running Minimal FHE Models
```bash
# Run with FHE simulation
python fhe_nanogpt_concrete_v3.py --fhe_mode simulate

# Run with actual encryption
python fhe_nanogpt_concrete_v3.py --fhe_mode execute
```

## Technical Details

### FHE Compatibility Features
1. Bit-width Control:
   - Explicit scaling between operations (x * 0.1)
   - Modified residual connections (averaging instead of addition)
   - Small weight initialization (std=0.005)

2. FHE-Friendly Operations:
   - Polynomial activation (xÂ² + x) instead of ReLU/GELU
   - Simplified attention mechanism
   - Element-wise operations where possible
   - Aggressive quantization (3-bit)

### Current Limitations
1. Maximum model size tested: 0.13M parameters
2. Input must be one-hot encoded for FHE circuit
3. Generation is currently token-by-token
4. Limited context window (32 tokens)

## Future Work
1. Scaling to larger model sizes
2. Implementing efficient token generation
3. Testing with real-world datasets
4. Optimizing FHE circuit compilation
5. Reducing encryption overhead

## Results
Our FHE-compatible GPT model achieves:
- Perfect accuracy between regular and FHE inference
- Successful compilation with 3-bit quantization
- Working encryption and decryption
- Stable numerical behavior through multiple layers