defaults:
  # - optim: adam
  # - optim: custom_adamw # Noise false
  - optim: custom_adamw_variant # Noise true
  # - optim: SGD
  - tasks:
      # - cola
      # - mnli
      # - mrpc
      # - qnli
      # - qqp
      - rte
      # - sst2
      # - stsb

user_name: 
saving_task:
save_train_data:

valid_tasks:

optim:
  lr: 4e-5

lora_rank: 2
lora_alpha: 1

grad_noise: 
grad_noise_scale: 10

experiment_float64: False

dropout_on: True

approx_sqrt_inverse: False
div_max: 500

softmax_temperature: 1
tanh_temperature: 1

epochs: 5

# These options are only used for scheduling:
warmup_steps: 0
cooldown_steps: 0
steps:

name: GLUE
evaluation_set: validation # always keep this at validation except for the final run

# checkpoint name:
# This can be either "latest", or a reference to a specific checkpoint in a subfolder
checkpoint: latest
ckpt_num: 0
path: ${impl.path} # Path for caches of datasets and tokenizers
max_seq_length: 128

# Default options:
# These can be overwritten by specific tasks
batch_size: 16
# batch_size: 1
batch_size_ramp: 0

gradient_clipping:
# limited_decay_keys: [bias, LayerNorm.bias, LayerNorm.weight, norm]
limited_decay_keys: []
scheduler: cosine-decay
# scheduler: budget-constant
budget: 100
optim_mod:
  name: none
eval_in_train_mode: True

testing:
  batch_size: 128

arch_modifications:
  classification_head:
    pooler: zero_index
    # pooler: avg
    include_ff_layer: True
    # head_dim: ${arch.hidden_size}
    nonlin: Tanh
    # nonlin: Tanh_poly_11
    # classifier_dropout: ${arch.hidden_dropout_prob}

task_now: 